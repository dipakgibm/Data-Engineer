{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ETL Job.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXa2gU09UE0a",
        "outputId": "42a68e09-30d6-411c-ee18-4228f51cdced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully completed the ETL Job\n"
          ]
        }
      ],
      "source": [
        "import glob                         # this module helps in selecting files \n",
        "import pandas as pd                 # this module helps in processing CSV files\n",
        "import xml.etree.ElementTree as ET  # this module helps in processing XML files.\n",
        "from datetime import datetime\n",
        "\n",
        "# Download Files\n",
        "\n",
        "#!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0221EN-SkillsNetwork/labs/module%206/Lab%20-%20Extract%20Transform%20Load/data/source.zip\n",
        "\n",
        "\n",
        "tmpfile    = \"temp.tmp\"               # file used to store all extracted data\n",
        "logfile    = \"logfile.txt\"            # all event logs will be stored in this file\n",
        "targetfile = \"transformed_data.csv\"   # file where transformed data is stored\n",
        "\n",
        "### CSV Extract Function\n",
        "def extract_from_csv(file_to_process):\n",
        "    dataframe = pd.read_csv(file_to_process)\n",
        "    return dataframe\n",
        "\n",
        "#JSON Extract Function\n",
        "def extract_from_json(file_to_process):\n",
        "    dataframe = pd.read_json(file_to_process,lines=True)\n",
        "    return dataframe\n",
        "\n",
        "# XML Extract Function\n",
        "def extract_from_xml(file_to_process):\n",
        "    dataframe = pd.DataFrame(columns=[\"name\", \"height\", \"weight\"])\n",
        "    tree = ET.parse(file_to_process)\n",
        "    root = tree.getroot()\n",
        "    for person in root:\n",
        "        name = person.find(\"name\").text\n",
        "        height = float(person.find(\"height\").text)\n",
        "        weight = float(person.find(\"weight\").text)\n",
        "        dataframe = dataframe.append({\"name\":name, \"height\":height, \"weight\":weight}, ignore_index=True)\n",
        "    return dataframe\n",
        "\n",
        "#Extract Function\n",
        "def extract():\n",
        "    extracted_data = pd.DataFrame(columns=['name','height','weight']) # create an empty data frame to hold extracted data\n",
        "    \n",
        "    #process all csv files\n",
        "    for csvfile in glob.glob(\"*.csv\"):\n",
        "        extracted_data = extracted_data.append(extract_from_csv(csvfile), ignore_index=True)\n",
        "        \n",
        "    #process all json files\n",
        "    for jsonfile in glob.glob(\"*.json\"):\n",
        "        extracted_data = extracted_data.append(extract_from_json(jsonfile), ignore_index=True)\n",
        "    \n",
        "    #process all xml files\n",
        "    for xmlfile in glob.glob(\"*.xml\"):\n",
        "        extracted_data = extracted_data.append(extract_from_xml(xmlfile), ignore_index=True)\n",
        "        \n",
        "    return extracted_data\n",
        "\n",
        "#Transform the data\n",
        "def transform(data):\n",
        "        \n",
        "        #Convert inches to meters and round off to two decimals(one inch is 0.0254 meters)\n",
        "        data['height'] = round(data.height * 0.0254,2)\n",
        "      \n",
        "        #Convert pounds to kilograms and round off to two decimals(one pound is 0.45359237 kilograms)\n",
        "        data['weight'] = round(data.weight * 0.45359237,2)\n",
        "        return data\n",
        "\n",
        "#Loading \n",
        "def load(targetfile,data_to_load):\n",
        "    data_to_load.to_csv(targetfile)  \n",
        "\n",
        "#Logging\n",
        "def log(message):\n",
        "    timestamp_format = '%Y-%h-%d-%H:%M:%S' # Year-Monthname-Day-Hour-Minute-Second\n",
        "    now = datetime.now() # get current timestamp\n",
        "    timestamp = now.strftime(timestamp_format)\n",
        "    with open(\"logfile.txt\",\"a\") as f:\n",
        "        f.write(timestamp + ',' + message + '\\n')\n",
        "\n",
        "# ETL job running\n",
        "log(\"ETL Job Started\")\n",
        "log(\"Extract phase Started\")\n",
        "extracted_data = extract()\n",
        "log(\"Extract phase Ended\")\n",
        "extracted_data\n",
        "\n",
        "log(\"Transform phase Started\")\n",
        "transformed_data = transform(extracted_data)\n",
        "log(\"Transform phase Ended\")\n",
        "transformed_data \n",
        "\n",
        "log(\"Loading phase Started\")\n",
        "transformed_data = transform(extracted_data)\n",
        "log(\"Loading phase Ended\")\n",
        "load(targetfile,transformed_data)\n",
        "print(\"Successfully completed the ETL Job\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "Bgc6W5PeWsQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat transformed_data.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I61-myiRUitV",
        "outputId": "05ad01e1-b927-4c52-ca9d-7d883b5c1e39"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ",name,height,weight\n",
            "0,alex,0.04,23.25\n",
            "1,ajay,0.05,28.08\n",
            "2,alice,0.04,31.48\n",
            "3,ravi,0.04,29.28\n",
            "4,joe,0.04,29.69\n",
            "5,alex,0.04,23.25\n",
            "6,ajay,0.05,28.08\n",
            "7,alice,0.04,31.48\n",
            "8,ravi,0.04,29.28\n",
            "9,joe,0.04,29.69\n",
            "10,alex,0.04,23.25\n",
            "11,ajay,0.05,28.08\n",
            "12,alice,0.04,31.48\n",
            "13,ravi,0.04,29.28\n",
            "14,joe,0.04,29.69\n",
            "15,jack,0.04,25.37\n",
            "16,tom,0.04,29.11\n",
            "17,tracy,0.05,28.08\n",
            "18,john,0.04,23.12\n",
            "19,jack,0.04,25.37\n",
            "20,tom,0.04,29.11\n",
            "21,tracy,0.05,28.08\n",
            "22,john,0.04,23.12\n",
            "23,jack,0.04,25.37\n",
            "24,tom,0.04,29.11\n",
            "25,tracy,0.05,28.08\n",
            "26,john,0.04,23.12\n",
            "27,simon,0.04,23.12\n",
            "28,jacob,0.04,24.83\n",
            "29,cindy,0.04,26.22\n",
            "30,ivan,0.04,23.48\n",
            "31,simon,0.04,23.12\n",
            "32,jacob,0.04,24.83\n",
            "33,cindy,0.04,26.22\n",
            "34,ivan,0.04,23.48\n",
            "35,simon,0.04,23.12\n",
            "36,jacob,0.04,24.83\n",
            "37,cindy,0.04,26.22\n",
            "38,ivan,0.04,23.48\n"
          ]
        }
      ]
    }
  ]
}